# __Incremental Materialization__

## __Introduction__

Incremental materialization is a method of updating data in a table by only adding or modifying the rows that have changed since the last update. Instead of rebuilding the entire table, which is both time-consuming and resource-intensive, incremental materialization focuses on the deltas - the changes.

## __Importance__

-   ***Efficiency in data processing***: It reduces the volume of data to be processed during each run.
-   ***Cost reduction***: By processing less data, you consume fewer compute resources, thus lowering the costs.
-   ***Timeliness of data***: It allows for more frequent updates, ensuring that your data is as current as possible.

## __Strategy__

<figure markdown="1">

```mermaid
graph LR
A[SOURCE TABLE] -->|Incremental Materialization| B[TARGET TABLE]
```

</figure>

There are 2 common types of incremental materialization:

-   ***Time-based incremental materialization***: This is the most straightforward approach, where new data is selected and processed based on a time column (like a timestamp). This method is ideal for event tables where new data is continuously generated. For example, only processing rows where the timestamp is greater than the last update.
-   ***Change data capture (CDC) incremental materialization***: This method relies on identifying and capturing changes made to the source data (inserts, updates, deletes) and applying these changes to the target dataset. While this technique may present a higher level of complexity in its implementation, it is particularly beneficial in scenarios where relying on a time column is not feasible or accurate, such as instances where older timestamps are recorded after newer ones. In relation to Snowflake, a specific feature known as 'streams' exists, which will be explored in greater detail in a subsequent section.

The previous implementation mostly utilized table materialization, a process that could take several hours for extensive tables. Additionally, it applied a basic form of incremental materialization. However, this method lacked a robust strategy for determining the correct incremental cutoff, resulting in missing data from the target table.

To address these issues, we significantly revised the logic of the existing models, placing a strong emphasis on adopting incremental materialization for all future models. After implementing incremental materialization on large tables, our team observed substantial improvements in performance. Here's a summary of the results:

| TABLE_NAME                      | WH_SIZE | OLD_RUN_TIME (SEC) | OLD_RUN_TIME (MIN) | NEW_RUN_TIME (SEC) | NEW_RUN_TIME (MIN) | PERCENTAGE_CHANGE |
|---------------------------------|---------|--------------------|--------------------|--------------------|--------------------|-------------------|
| base_shopify_order              | XS      | 2400               | 40.00              | 60                 | 1.00               | -97.5%            |
| fct_mparticle_ecommerce_product | M       | 2700               | 45.00              | 5                  | 0.08               | -99.8%            |
| dim_software_modelname          | M       | 3600               | 60.00              | 10                 | 0.17               | -99.7%            |
| dim_software_user               | M       | 4200               | 70.00              | 60                 | 1.00               | -98.6%            |
| mart_software_agg_event_other   | M       | 4800               | 80.00              | 180                | 3.00               | -96.3%            |
| base_ga4                        | M       | 5400               | 90.00              | 180                | 3.00               | -96.7%            |

!!! note

    Just a few tables are shown as examples.

## __Code implementation__

### __Defining models__

Incremental models are defined with select statements, with the materialization defined in a config block. You also need to tell dbt how to filter the rows on an incremental run. This is usually done by filtering the source table with the timestamp that is greater than the latest updated timestamp of the target table.

```sql title="model.sql" linenums="1" hl_lines="3"
{{
    config(
        materialized='incremental'
    )
}}

select ...
```

Here are the other configurations that you can define:

| Parameter            | Default value | Expected values                                    |
|----------------------|---------------|----------------------------------------------------|
| incremental_strategy | append        | append, merge, delete+insert                       |
| unique_key           | null          | a list of columns ['col1', 'col2', …]              |
| on_schema_change     | ignore        | ignore, append_new_columns, sync_all_columns, fail |
		
=== "Incremental strategies"

    | Value         | Description                                                                                                   |
    |---------------|---------------------------------------------------------------------------------------------------------------|
    | append        | Each incremental run appends to the table.                                                                    |
    | merge         | Each incremental run merges new rows with the existing rows.                                                  |
    | delete+insert | Each incremental run delete records in the target table that are to be updated, and then an insert statement. |


=== "Unique key"

    | Behaviour if not provided                    | Behaviour if provided                                                 |
    |----------------------------------------------|-----------------------------------------------------------------------|
    | Incremental runs are behaving as append-only | merge / delete+insert is going to be run on the given columns’ names. |

    !!! note

        This is only required for “merge” & “delete+insert”
        
=== "Schema changes"

    | Value              | Description                                       |
    |--------------------|---------------------------------------------------|
    | ignore             | Ignore schema changes.                            |
    | append_new_columns | Only add the new fields; keep the removed fields. |
    | sync_all_columns   | Full sync schema changes.                         |
    | fail               | Fails when the schema change is detected.         |

Below is an example incremental materialization with the merge strategy on the row_id column while also ignoring any changes in the schema in every run:

```sql title="model.sql" linenums="1"
{{
    config(
        materialized='incremental'
        ,incremental_strategy = 'merge'
        ,unique_key = ['row_id']
        ,on_schema_change = 'ignore'
    )
}}

select

    row_id
    ,event_timestamp
    ,...

from {{ ref('source_table') }}

{% if is_incremental() %}

  -- this filter will only be applied on an incremental run
  where event_timestamp > (select max(event_timestamp) from {{ this }})

{% endif %}
```

### __Creating get incremental cutoff macro__ 

In the initial approach, acquiring the maximum event timestamp of the target table involves using a common table expression (CTE) that refers to the table itself, wrapped within an is_incremental() macro. This setup ensures the first run doesn't fail by referencing a non-existent table.

However, this method recommended by dbt isn't the most efficient. The process of checking the max event timestamp inside a CTE can slow down the source table scan compared to using a constant. Moreover, the readability of the where clause wrapped in is_incremental() macro could be improved.

To resolve these issues, we've developed a macro named “get_incremental_cutoff”, which efficiently retrieves the required incremental timestamp and stores it in a variable.

```sql title="dbt_project/macros/get_incremental_cutoff.sql" linenums="1"
{% macro get_incremental_cutoff(target_table, time_column, lag_time, lag_part, table_start_time, timestamp_format='timestamp_ntz') -%}

    {% if is_incremental() -%}

        {% set sql -%}

            select max({{ time_column }}) - interval '{{ lag_time }} {{ lag_part }}' 
            
            from {{ target_table }}
            
        {% endset -%}

        '{{ run_query(sql).columns[0].values()[0] }}'::{{ timestamp_format }}

    {%- else -%}

        '{{ table_start_time }}'::{{ timestamp_format }}

    {%- endif %}

{%- endmacro %}
```

This macro comes with the following added benefits:

-   ***Improved scan time***: Utilizing a variable in the where clause instead of a CTE speeds up the scanning time.
-   ***Specifying table start time***: The table_start_time parameter allows you to bypass the is_incremental() macro for the initial run, using a default timestamp.
-   ***Dynamic interval difference***: You can dynamically retrieve an older incremental timestamp, which is especially beneficial for merges in scenarios where recent data might not yet be reliable.
-   ***Custom timestamp format***: This feature is useful when the timestamp formats of the source and target tables differ. For instance, if the source table stores timestamps in a varchar format and the target table stores actual timestamps.

Below are examples of 2 incremental scenarios with different requirements for the incremental cutoff:

-   ***Scenario 1***: Incremental timestamp is the max event_timestamp in the target table
-   ***Scenario 2***: Incremental timestamp is one day prior to the max event_timestamp in the target table

=== "Scenario 1"

    ```sql title="model.sql" linenums="1"
    {{
        config(
            materialized='incremental'
            ,incremental_strategy = 'append'
            ,on_schema_change = 'ignore'
        )
    }}

    {% set incremental_cutoff = get_incremental_cutoff(this, 'event_timestamp', 0, 'hour', '2022-09-15') %}

    select

        row_id
        ,event_timestamp
        ,...

    from {{ ref('source_table') }}

    where event_timestamp > {{ incremental_cutoff }}
    ```

=== "Scenario 2"

    ```sql title="model.sql" linenums="1"
    {{
        config(
            materialized='incremental'
            ,incremental_strategy = 'append'
            ,on_schema_change = 'ignore'
            ,pre_hook = "
                {% set incremental_cutoff = get_incremental_cutoff(this, 'event_timestamp', 1, 'day', '2022-09-15') %}

                {% if is_incremental() %}

                    delete from {{ this }}
                    where sf_created_at_timestamp >= {{ incremental_cutoff }}

                {% endif %}
            "
        )
    }}

    {% set incremental_cutoff = get_incremental_cutoff(this, 'event_timestamp', 1, 'day', '2022-09-15') %}

    select

        row_id
        ,event_timestamp
        ,...

    from {{ ref('source_table') }}

    where event_timestamp >= {{ incremental_cutoff }}
    ```

In the second scenario, we move away from a merge strategy, which tends to be more resource-intensive due to the necessity of scanning the entire table. This is often the case as the unique key is typically not clustered. Instead, we adopt a more streamlined method that involves deleting existing data in the pre-hook and then appending new data in the model. It's crucial to understand that this method differs significantly from the "delete+insert" incremental_strategy defined in the config. The latter is more similar to a “merge” process as it requires a unique key and might necessitate an extensive table scan.

This alternative approach is particularly useful in scenarios where there is a delay in data availability or instances of back-dated data. For instance, data meant for today might only be fully available by tomorrow. Reprocessing the previous day’s data in such situations ensures that the dataset remains complete and accurate, justifying the need for this second scenario.

It's also important to highlight that the macro used in this process will be evaluated before the execution of the model, irrespective of whether it's placed in the pre-hook. This ensures that the incremental_cutoff value remains consistent across different script sections like the pre-hook, the model itself, and the post-hook. However, it's essential to note that these sections are treated as distinct scripts, which means that reusing variables across them is not feasible.

### __Working with aggregated tables__

Consider the following example where we have a fact_sales that will be used to create a dim_user table. 

For the first run, all data from fact_sales should be scanned and aggregated. The dim_user table should materialize correctly:

=== "fact_sales (1st run)"

    | event_date | user_id | email       | order_revenue |
    |------------|---------|-------------|---------------|
    | 2020-01-01 | a       | a@gmail.com | 5             |
    | 2020-01-02 | a       | a@gmail.com | 5             |
    | 2020-01-02 | b       | b@gmail.com | 20            |

=== "dim_user (1st run)"

    | user_id | email       | ltv | first_event_date | last_event_date |
    |---------|-------------|-----|------------------|-----------------|
    | a       | a@gmail.com | 10  | 2020-01-01       | 2020-01-02      |
    | b       | b@gmail.com | 20  | 2020-01-02       | 2020-01-02      |

For the second run, however, only new data from fact_sales are scanned and aggregated. The dim_user table materialize incorrectly

=== "fact_sales (2nd run)"

    | event_date     | user_id | email            | order_revenue |
    |----------------|---------|------------------|---------------|
    | 2020-01-01     | a       | a@gmail.com      | 5             |
    | 2020-01-02     | a       | a@gmail.com      | 5             |
    | 2020-01-02     | b       | b@gmail.com      | 20            |
    | ==2020-01-03== | ==a==   | ==a2@gmail.com== | ==5==         |
    | ==2020-01-03== | ==c==   | ==c@gmail.com==  | ==10==        |

=== "dim_user (2nd run)"

    | user_id | email            | ltv    | first_event_date | last_event_date |
    |---------|------------------|--------|------------------|-----------------|
    | ==a==   | ==a2@gmail.com== | ==5==  | ==2020-01-03==   | ==2020-01-03==  |
    | b       | b@gmail.com      | 20     | 2020-01-02       | 2020-01-02      |
    | ==c==   | ==c.@gmail.com== | ==10== | ==2020-01-03==   | ==2020-01-03==  |

This result incorrectly omits historical data for "user a," causing inaccuracies in "ltv" and "first_event_date". To address this, we can employ two strategies:

-   ***Method 1*** - Scan and Recalculate: This involves scanning all records associated with the unique key (user_id) from the current incremental batch and recalculating the aggregated metrics.
-   ***Method 2*** - Union and Update: This strategy involves taking a union of the old status and the updated status for each unique key in the incremental batch and then checking which status should be valid.

=== "Method 1"

    ```sql title="model.sql" linenums="1"
    {{
        config(
            materialized = 'incremental'
            ,incremental_strategy='merge'
            ,unique_key = ['user_id']
        )
    }}

    {% set incremental_cutoff = get_incremental_cutoff(this, 'last_event_date', 0, 'day', '2020-01-01') %}

    with source as(

        select * from {{ ref('source_table') }}
    
        {% if is_incremental() %}

        where user_id in(

                select distinct user_id

                from {{ ref('source_table') }}

                where event_date > {{ incremental_cutoff }}
            )

        {% endif %}

    ), final as (

        select

            user_id
            ,max_by(email, event_date)           as email
            ,sum(order_revenue)                  as ltv
            ,min(event_date)                     as first_event_date
            ,max(event_date)                     as last_event_date

        from source

        group by all

    )

    select * from final
    ```

    ⇨ This method works by filtering the source data to include only those users (user_id) who have new data since the last run, as determined by the incremental_cutoff variable. For these filtered users, it recalculates all aggregate metrics like ltv, first_event_date, etc.

=== "Method 2"

    ```sql title="model.sql" linenums="1"
    {{
        config(
            materialized = 'incremental'
            ,incremental_strategy='merge'
            ,unique_key = ['user_id']
        )
    }}

    {% set incremental_cutoff = get_incremental_start_time_exact(this, 'last_event_date', 0, 'day', '2020-01-01') %}

    with source as(

        select * from {{ ref('source_table') }}

        where event_date > {{ incremental_cutoff }}

    ), tmp as (

        select

            user_id
            ,max_by(email, event_date)           as email
            ,sum(order_revenue)                  as ltv
            ,min(event_date)                     as first_event_date
            ,max(event_date)                     as last_event_date

        from source

        group by all

    ), combined as (

        select * from tmp

        {% if is_incremental() %}

        union all

        select a.* from {{ this }} a

        inner join tmp b
        on a.user_id = b.user_id

        {% endif %}

    ), final as (

        select

            user_id
            ,max_by(email, last_event_date)      as email
            ,sum(ltv)                            as ltv
            ,min(first_event_date)               as first_event_date
            ,max(last_event_date)                as last_event_date

        from combined

        group by all
        
    )
    
    select * from final
    ```

    ⇨ This method works by selecting data from the source table that is newer than the incremental_cutoff. It then aggregates this new data to calculate aggregated metrics for these users in this particular period. Then it will union this new aggregated data with existing data in the target table ({{ this }}) for the same users. Finally, it re-aggregates this combined data to update the metrics (i.e. sum for ltv, max & min for event_date, etc.). 

These methods both ensure that the final metrics consider both the historical data from the existing records and the new data. However there are some difference:

|             | Method 1                                                                                                                                                                                                                  | Method 2                                                                                                                                                                                                                                                           |
|-------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Complexity  | Less complex. It only involves filtering and aggregating.                                                                                                                                                                 | More complex. It involves filtering, aggregating & unioning.                                                                                                                                                                                                       |
| Performance | Less performant. It necessitates a full table scan, which can be inefficient, especially when the unique key (often a varchar) is not clustered. Aggregating a larger subset of data can also be more resource-intensive. | More performant. It only needs to scan the most recent data, working with a smaller subset for aggregation. Despite needing to perform aggregation twice, it tends to be faster than Method 1 because it avoids the full table scan and handles less data overall. |