# __Overview__

## __Architecture of dbt__

### __Services__

- ***dbt Core***: The open-source command-line tool that executes your dbt projects. It's responsible for compiling your dbt models (written in SQL and Jinja) into runnable SQL, executing those models on your data warehouse, and handling dependencies between models.
- ***dbt Cloud***: An optional web-based application that provides a user interface for managing dbt projects, including orchestration, scheduling, logging, and documentation generation.

In the past, my team utilized dbt Cloud for scheduling model runs on Snowflake but has since transitioned to using Dagster for integration and orchestration, discontinuing our dbt Cloud subscription to reduce costs. This approach is particularly beneficial if you have existing infrastructure and orchestration tools. For guidance on integrating dbt with Dagster, refer to my comprehensive guide available [here](https://logantran26.github.io/data_with_logan/dagster/).

### __Workflow Components__

- ***Models***: The central concept in dbt, models are SQL queries that define the transformations you want to perform on your raw data to produce analytics-ready tables. Models can depend on other models, allowing dbt to resolve dependencies and execute SQL in the correct order.
- ***Sources***: dbt allows you to declare your raw data tables as sources. This helps in documenting where your data is coming from and enables dbt to check for freshness and validity.
- ***Tests***: dbt supports data testing by allowing you to define assertions on your data, ensuring its quality and integrity. These tests can be run as part of your deployment process.
- ***Packages***: dbt supports modularization through packages, which are reusable collections of models, tests, macros, and so on. You can include community-maintained packages in your project to extend its capabilities.
- ***Macros***: Macros are snippets of reusable code written in Jinja. They allow you to abstract complex logic or SQL patterns, making your models more maintainable.
- ***Documentation***: dbt automatically generates documentation for your project based on your model definitions, tests, and descriptions. This documentation is a valuable resource for understanding your data transformations and lineage.

### __Materializations__

dbt supports different ways of materializing your SQL models in the data warehouse:

- ***View***: The default materialization, where the model is represented as a SQL view.
- ***Table***: Materializes the model as a physical table in your warehouse, useful for large datasets or performance-intensive queries.
- ***Incremental***: Updates or appends data to an existing table based on a specified logic, optimizing for performance and cost.
- ***Ephemeral***: These models are compiled into the SQL of dependent models and are not directly materialized in the warehouse, useful for intermediate transformations.

You can also define custom materializations. For guidance on creating an incremental stream for Snowflake, refer to my comprehensive guide available [here](https://logantran26.github.io/data_with_logan/snowflake/cost_optimization/3.snowflake_stream/).

## __dbt's project structure__

A dbt project typically serves two main functions:

- ***Data transformation***: Utilizing dbt models to transform raw data into analytics-ready formats within a data warehouse.
- ***Data testing and documentation***: Ensuring data integrity and generating documentation for transformed data.

The following is a comprehensive breakdown of each folder and file in my dbt project setup:

``` yaml title="dbt project structure"
dbt_project/
├── data/                  # (1)!
├── macros/                # (2)!
├── models/                # (3)!
│   ├── source/  
│   ├── stage/ 
│   ├── common/  
│   ├── common_mart/    
│   ├── utilities/    
├── snapshots/             # (4)!
├── tests/                 # (5)!
├── dbt_project.yml        # (6)!
├── profiles.yml           # (7)!
├── packages.yml           # (8)!

```

1. For adding seed files (CSV) that contain static data used in transformations, such as mapping tables.
2. For creating usable Jinja templates for custom logic or SQL code that can be reused across models.
3. For creating SQL files defining transformations. Organized by domain or logical grouping
    - `source/`: For initial parsing, casting and renaming of raw data.
    - `stage/`: For cleaning raw data and preparing for dimensional analysis.
    - `common/`: For storing fact & dim tables.
    - `common_mart/`: For storing mart, report & bridge table.
    - `utilities/`: For storing utility scripts (i.e. cloning production, monthly update or deduplicate for tables, etc.)
4. For defining SQL queries on how to capture historical data changes over time.
5. For defining SQL queries that you can write to test the models and resources in your project.
6. Main configuration file for the dbt project, defining project-specific settings.
7. Defines connection details to your data warehouse, typically managed outside the project for security.
8. Manages dependencies on external dbt packages or modules.